#!/bin/bash
#SBATCH --job-name horovod
#SBATCH -D  /gpfs/home/sam14/sam14015/task/proj/
#SBATCH --output /gpfs/home/sam14/sam14015/task/proj/logs/_GPUS_._MODEL_.out
#SBATCH --error /gpfs/home/sam14/sam14015/task/proj/logs/_GPUS_._MODEL_.err
#SBATCH --nodes _NODES_
#SBATCH --tasks-per-node _TASKS_PER_NODE_
#SBATCH --cpus-per-task 40
#SBATCH --gres gpu:_TASKS_PER_NODE_
#SBATCH --time 0:10:00
#SBATCH --qos debug

#MODEL=vgg_16
#MODEL=resnet_v2_101
MODEL="_MODEL_"
DATASET=cifar10
DIR="/gpfs/home/sam14/sam14015/task/proj"
PROCS="_GPUS_"

BATCH_SIZE=64
NUM_CLASSES=10
LEARNING_RATE=0.0001
LOG_N_STEPS=10
#SAVE_N_STEPS=50
MAX_N_STEPS=200
#MAX_N_STEPS=750

source "$DIR/init_gpu_p9.sh"

T0=$(date +%s)

rm -rf "$DIR/models/${DATASET}_${MODEL}.$PROCS"

mpirun -np $PROCS \
	--bind-to none \
	--map-by slot \
	-x NCCL_DEBUG=INFO \
	-x LD_LIBRARY_PATH \
	-x PATH \
	-x HOROVOD_MPI_THREADS_DISABLE=1 \
	--mca pml ob1 \
	--mca btl ^openib \
	python tf_estimator_horovod.py \
	--dataset_name=$DATASET \
	--model_name="$MODEL" \
	--train_dir="$DIR/datasets/data/$DATASET" \
	--eval_dir="$DIR/datasets/data/$DATASET" \
	--model_dir="$DIR/models/${DATASET}_${MODEL}.$PROCS" \
	--num_classes=$NUM_CLASSES \
	--batch_size=$BATCH_SIZE \
	--learning_rate=$LEARNING_RATE \
	--log_every_n_steps=$LOG_N_STEPS \
	--max_number_of_steps=$MAX_N_STEPS
#	--save_summary_steps=$SAVE_N_STEPS \

T1=$(date +%s)

rm -rf "$DIR/models/${DATASET}_${MODEL}.$PROCS"

echo $PROCS $(($T1 - $T0)) $MODEL $DATASET >> $DIR/data.csv

./measure-times.sh "$DIR/logs/${PROCS}.${MODEL}.err" \
	> "$DIR/logs/time.${PROCS}.${MODEL}.csv"
